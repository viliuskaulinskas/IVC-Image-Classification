{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import os, os.path\n",
    "import random\n",
    "from skimage import io, transform\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label: cat == 0; dog == 1\n",
    "\n",
    "# the number of cat/dog pics for each breed\n",
    "NUM_CATS = [99, 98, 100, 100, 100, 92, 100, 100, 99, 100, 100, 100]\n",
    "NUM_DOGS = [100 for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a dataset class to streamline the training, validation and testing\n",
    "# 3-fold cross validation\n",
    "# A&B -> C, then repeat for A&C -> B and B&C -> A (according to an email from Bob)\n",
    "# data is split into 3 equal parts: A, B, C\n",
    "# training and validation set is split 75%/25%\n",
    "\n",
    "class CatDogDataset(Dataset):\n",
    "    def __init__(self, root_dir, set_name, transform=None):\n",
    "        # root directory of the dataset is usually '../catdog/'\n",
    "        self.root_dir = root_dir\n",
    "        # set_name is used to identify which part (A, B or C) is testing set\n",
    "        # and determine the output of the dataset is training/validation/test\n",
    "        self.set_name = set_name\n",
    "        # apply transform if needed\n",
    "        self.transform = transform\n",
    "        \n",
    "        # rng (random number generator) is used to generate random indices for part A, B and C\n",
    "        # we use a seed to make sure the split is fixed in each run\n",
    "        rng = np.random.RandomState(seed=0)\n",
    "        \n",
    "        # initialise empty lists to store names for different image sets\n",
    "        # we will use the names of images to retrieve images later\n",
    "        cat_A, cat_B, cat_C, dog_A, dog_B, dog_C = [], [], [], [], [], []\n",
    "        \n",
    "        # we use for loop to sample images from each breed of dogs and cats\n",
    "        # to make sure set A, B and C have the same data distribution\n",
    "        for b in range(1, 13):\n",
    "            \n",
    "            # here we sample image indices for each breed\n",
    "            # for each breed (around 100 images), sample 66 images for set catA and catB\n",
    "            catAncatB = rng.choice(a=[i for i in range(NUM_CATS[b-1])], size=66, replace=False)\n",
    "            \n",
    "            # catA samples 33 images from combined catAncatB set\n",
    "            catA = rng.choice(a=catAncatB, size=33, replace=False)\n",
    "            \n",
    "            # the rest of images in combined catAncatB set will go to catB\n",
    "            catB = [i for i in catAncatB if i not in catA]\n",
    "            \n",
    "            # the rest of images in the original set will go to catC\n",
    "            catC = [i for i in range(NUM_CATS[b-1]) if i not in catAncatB]\n",
    "            \n",
    "            # use the image indices to build the name/path of the images in each set\n",
    "            # 0 is the label for cats\n",
    "            cat_A += [['CATS/cat_'+str(b)+'_'+str(i)+'.png', 0] for i in catA]\n",
    "            cat_B += [['CATS/cat_'+str(b)+'_'+str(i)+'.png', 0] for i in catB]\n",
    "            cat_C += [['CATS/cat_'+str(b)+'_'+str(i)+'.png', 0] for i in catC]\n",
    "            \n",
    "            \n",
    "            # it is the same process but for dogs\n",
    "            # 1 is the label for dogs\n",
    "            dogAndogB = rng.choice(a=[i for i in range(NUM_DOGS[b-1])], size=66, replace=False)\n",
    "            dogA = rng.choice(a=dogAndogB, size=33, replace=False)\n",
    "            dogB = [i for i in dogAndogB if i not in dogA]\n",
    "            dogC = [i for i in range(NUM_DOGS[b-1]) if i not in dogAndogB]\n",
    "            \n",
    "            dog_A += [['DOGS/dog_'+str(b)+'_'+str(i)+'.png', 1] for i in dogA]\n",
    "            dog_B += [['DOGS/dog_'+str(b)+'_'+str(i)+'.png', 1] for i in dogB]\n",
    "            dog_C += [['DOGS/dog_'+str(b)+'_'+str(i)+'.png', 1] for i in dogC]\n",
    "        \n",
    "        # combine cat and dog images to form complete set A, B and C, and reshuffle\n",
    "        catdogA = cat_A + dog_A\n",
    "        catdogB = cat_B + dog_B\n",
    "        catdogC = cat_C + dog_C\n",
    "        # use 0 as the seed to make sure the reproducibility\n",
    "        random.Random(0).shuffle(catdogA)\n",
    "        random.Random(0).shuffle(catdogB)\n",
    "        random.Random(0).shuffle(catdogC)\n",
    "        \n",
    "        # return a dataset depending on the set_name\n",
    "        # first uppercase letter represents the test dataset in the current split\n",
    "        # train/val/test represents the dataset returned in the current split\n",
    "        \n",
    "        if self.set_name == 'C_train':\n",
    "            # when set C is the test set, set A and B combined will be training and validation set\n",
    "            self.data = catdogA + catdogB\n",
    "            random.Random(1).shuffle(self.data)\n",
    "            # with 75%/25% split, first 1200 images will be training set\n",
    "            self.data = self.data[:1200]\n",
    "        elif self.set_name == 'C_val':\n",
    "            self.data = catdogA + catdogB\n",
    "            random.Random(1).shuffle(self.data)\n",
    "            # the rest of it will be validation set\n",
    "            self.data = self.data[1200:]\n",
    "        elif self.set_name == 'C_test':\n",
    "            self.data = catdogC\n",
    "        elif self.set_name == 'B_train':\n",
    "            self.data = catdogA + catdogC\n",
    "            random.Random(1).shuffle(self.data)\n",
    "            self.data = self.data[:1200]\n",
    "        elif self.set_name == 'B_val':\n",
    "            self.data = catdogA + catdogC\n",
    "            random.Random(1).shuffle(self.data)\n",
    "            self.data = self.data[1200:]\n",
    "        elif self.set_name == 'B_test':\n",
    "            self.data = catdogB\n",
    "        elif self.set_name == 'A_train':\n",
    "            self.data = catdogB + catdogC\n",
    "            random.Random(1).shuffle(self.data)\n",
    "            self.data = self.data[:1200]\n",
    "        elif self.set_name == 'A_val':\n",
    "            self.data = catdogB + catdogC\n",
    "            random.Random(1).shuffle(self.data)\n",
    "            self.data = self.data[1200:]\n",
    "        elif self.set_name == 'A_test':\n",
    "            self.data = catdogA\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image_name, target = self.data[index][0], self.data[index][1]\n",
    "        img = io.imread(self.root_dir + image_name)\n",
    "        \n",
    "        # there is a grayscale image in the cat images\n",
    "        # we will convert it to a colour image by duplicating it across RGB channels\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n",
    "            \n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, np.array([target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation is limited to translation, rotation, flipping and scale variations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # apply random flips to make networks more robust\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(degrees=(-90, 90)),\n",
    "        transforms.ToTensor(),\n",
    "        # normalise the images\n",
    "        # the first tuple is the mean in each channel, the second one is the std\n",
    "        # these numbers are pre-calculated in https://pytorch.org/docs/stable/torchvision/models.html\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(setname, batch_size=4, root_dir='../catdog/'):\n",
    "    image_datasets = {x: CatDogDataset(root_dir=root_dir, \n",
    "                                       set_name=setname+'_'+x, \n",
    "                                       transform=data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataloaders = {x: DataLoader(image_datasets[x], \n",
    "                                 batch_size = batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=2)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    \n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, model_path, num_epochs=50):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_running_corrects = 0\n",
    "    \n",
    "    acc_list_train = []\n",
    "    acc_list_val = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "        \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                labels = labels.squeeze(1)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                acc_list_train.append(epoch_acc)\n",
    "            elif phase == 'val':\n",
    "                acc_list_val.append(epoch_acc)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "        \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_running_corrects = running_corrects\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    torch.save(best_model_wts, model_path)\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, best_acc, best_running_corrects, dataset_sizes['val'], acc_list_train, acc_list_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in dataloaders['test']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        labels = labels.squeeze(1)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes['test']\n",
    "    epoch_acc = running_corrects.double() / dataset_sizes['test']\n",
    "\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        'test', epoch_loss, epoch_acc))\n",
    "    \n",
    "    return epoch_acc, running_corrects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Use A as test set\n",
      "####################\n",
      "####################\n",
      "Use B as test set\n",
      "####################\n",
      "####################\n",
      "Use C as test set\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-2\n",
    "batch_size = 16\n",
    "num_epochs = 25\n",
    "\n",
    "# use gpu if it is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# three-fold validation\n",
    "\n",
    "# use different set (A, B or C) as test set\n",
    "testsets = ['A', 'B', 'C']\n",
    "\n",
    "corrects_val = []\n",
    "val_sizes = []\n",
    "corrects_test = []\n",
    "\n",
    "for testset in testsets:\n",
    "    \n",
    "    print('#' * 20)\n",
    "    print('Use ' + testset + ' as test set')\n",
    "    print('#' * 20)\n",
    "    \n",
    "    dataloaders, dataset_sizes = load_data(setname=testset, batch_size=batch_size)\n",
    "    \n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    \n",
    "    # ‘freezing’ the convolutional layers except for the fully connected layer\n",
    "    for param in resnet18.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # when assigning a new layer, requires_grad will be set to True automatically\n",
    "    resnet18.fc = nn.Linear(512, 2)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(resnet18.parameters(), \n",
    "                                 lr=learning_rate, weight_decay=weight_decay)\n",
    "    model_path = './models/' + testset + '_astest'\n",
    "    \n",
    "    resnet18 = resnet18.to(device)\n",
    "    \n",
    "    # training\n",
    "    resnet18, acc_val, correct_val, val_size , acc_list_train, acc_list_val= train_model(resnet18, criterion, optimizer, \n",
    "                                                                            model_path=model_path, num_epochs=num_epochs)\n",
    "    \n",
    "    # test                                                                        \n",
    "    acc_test, correct_test = test_model(resnet18)\n",
    "    \n",
    "    corrects_val.append(correct_val)\n",
    "    corrects_test.append(correct_test)\n",
    "    val_sizes.append(val_size)\n",
    "    \n",
    "    # plot and store the graphs\n",
    "    \n",
    "    epoch_plot = [e for e in range(1, num_epochs+1)]\n",
    "    plt.plot(epoch_plot, acc_list_train, label='train')\n",
    "    plt.plot(epoch_plot, acc_list_val, label='val')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xticks([x for x in epoch_plot if x%5==0 ])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.savefig('./graphs/' + testset + '_astest.png')   \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "average_acc_val = sum(corrects_val).double() / sum(val_sizes)\n",
    "average_acc_test = sum(corrects_test).double() / 2388\n",
    "\n",
    "print('val average acc {:.4f} '.format(average_acc_val))\n",
    "print('test average acc {:.4f} '.format(average_acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for testset in testsets:\n",
    "    model = models.resnet18()\n",
    "    model.fc = nn.Linear(512, 2)\n",
    "    \n",
    "    model_path = './models/' + testset + '_astest'\n",
    "    model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers in the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
